{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final project exercises\n",
    "\n",
    "In the previous notebook we introduced questING, our final project for this course. In this notebook you will be asked to complete or implement some tests. Note that implementing some of the tests might require changes in the code, but we'll try to tell you upfront if code changes are expected.\n",
    "\n",
    "All the tests and code you might need to change are located in the ```final-project/tests``` and ```final-project/questing``` folders respectively. You can find the solutions inside the ```final-project-solutions```, following the same structure.\n",
    "\n",
    "## Goal\n",
    "\n",
    "The goal is not that your solution is exactly the same as the proposed solution. As it usually happens with any development, there are many ways to achieve the same thing, and although some might be more correct than others, it often comes down to a matter of taste too. So just make sure you understand the proposed solution and compare whether you are solving the same problem, and not only what option you prefer, but what would be more maintainable and readable. In this regard, the simplest solution is often the best.\n",
    "\n",
    "## Running the tests\n",
    "\n",
    "The tests are meant to be executed often, so feel free to run them as often as you want. If you're inside the ```final-project``` directory, you can run the test with coverage with this command:\n",
    "\n",
    "```bash\n",
    "pytest --cov-report term-missing --cov=questing tests\n",
    "```\n",
    "\n",
    "### Running a single test file\n",
    "\n",
    "In some cases, you might want to run just one of the test files, instead all of them. This is often the case for big projects, where the tests can start to take quite some time and you don't want to wait everytime you test. You can just provide the name of the file you want to test, instead of a folder:\n",
    "\n",
    "```bash\n",
    "pytest --cov-report term-missing --cov=questing tests/hero.py\n",
    "```\n",
    "\n",
    "### Running a single method from a file\n",
    "\n",
    "Finally, it could also be the case that you only want to run one of the methods you have in your file, not all of them. You can do that too, by providing the file name AND the method name, separated by a double colon:\n",
    "\n",
    "```bash\n",
    "pytest --cov-report term-missing --cov=questing tests/hero.py::test_hero_name\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "## Exercise - Add tests for the stats of the different heroes.\n",
    "\n",
    "In this first exercise, you need to add tests to make sure that the different stats of the heroes are correct. More precisely, you will have to add tests to check the following:\n",
    "\n",
    "- A Warrior has Power 4, Armor 1, Speed 1 and Health 10.\n",
    "- A Rogue has Power 3, Attack Range 3, Speed 2 and Health 9\n",
    "- A Mage has Power 3, Attack Range 2, Armor 2, Speed 1 and Health 8\n",
    "\n",
    "Once you're done, you can compare your solution with the one proposed. How many tests did you add? Would it be better to have more (or less tests)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "In the proposed solution, we have implemented one test per class, meaning that we have multiple assertions on every test. While this might be OK, in general the best practice is to try to have only one assertion per test, unless asserting one thing doesn't make sense without the other. So if you have create one test per class and attribute, your solution is probably better than the one provided.\n",
    "\n",
    "The only benefit of having a single test per class is readability, since someone reading the test can see all the expected attributes very easily in one go, instead of having to read multiple tests.\n",
    "\n",
    "Another idea would be to have a single, parametrized test, something like this:\n",
    "\n",
    "```python\n",
    "STATS_TO_CHECK = [\n",
    "    # Hero Class, attribute, expected_value\n",
    "    (Warrior, \"power\", 4),\n",
    "    (Warrior, \"armor\", 1),\n",
    "    # ...,\n",
    "    (Rogue, \"power\", 3),\n",
    "    (Rogue, \"attack_range\", 3),\n",
    "    # ...\n",
    "]\n",
    "\n",
    "@pytest.mark.parametrize(\"hero_cls,attribute,expected_value\", STATS_TO_CHECK)\n",
    "def test_hero_stats(hero_cls, attribute, expected_value):\n",
    "    hero = hero_cls(name=\"Test\")\n",
    "    \n",
    "    assert hasattr(hero, attribute), \\\n",
    "        f\"Hero class {hero_cls.__name__} should have an attribute '{attribute}'\"\n",
    "    actual_value = getattr(hero, attribute)\n",
    "    assert actual_value == expected_value, \\\n",
    "        f\"{hero_cls.__name__}'s {attribute} should be {expected_value}, but was {actual_value}\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "## Exercise 1 - Complete tests for the ```Position``` class\n",
    "\n",
    "For the first exercise, you will have to complete the tests inside ```tests/types_test.py```. At the end of the file, you will see that there are two tests to validate the distance between two positions. The idea is that you parametrize this test, using the ```pytest.mark.parametrize``` decorator.\n",
    "\n",
    "Click the button below for a hint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "This is how you can parametrize a test:\n",
    "\n",
    "```python\n",
    "import pytest\n",
    "\n",
    "MY_PARAMS = [\n",
    "    # num1, num2, expected_result\n",
    "    (1, 2, 3),\n",
    "    (3, 5, 8),\n",
    "]\n",
    "\n",
    "@pytest.mark.parametrize(\"x,y,expected_result\", MY_PARAMS)\n",
    "def test_sum(x, y, expected_result):\n",
    "    result = x + y\n",
    "    \n",
    "    assert result == expected_result, \\\n",
    "        f\"Result should have been {expected_result}, but was {result}\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "## Refactoring ```game_test.py```\n",
    "\n",
    "If you check the file where the game file is tested, you'll see that although the coverage is at 100%, the file with the tests, ```game.py``` is not that easy to read / maintain, the main reason being that there are quite a lot of tests, so it's not that easy to find your way.\n",
    "\n",
    "How would you refactor these tests so that they are easier to maintain? If you have a solution, feel free to go ahead. The only requirement: don't remove (and preferrably don't rename) tests.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "One option here could be to split the tests into multiple files, so for instance you could create a ```tests/game``` folder, and inside there have files with different tests (e.g. ```do_test.py```, ```available_actions_test.py```, etc... This way of going to a more granular level can make sense in some cases, but it can also be an indication that maybe the ```Game``` class is doing too many things and we should refactor the class itself.\n",
    "\n",
    "More in general, when something's hard to test or too complex, that usually means that the code is not right, and in the case, the ```Game``` class is probably more on that side."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 - Bug in the \"available actions\"\n",
    "\n",
    "You might have noticed in the previous notebook (or not), but there's a small bug in how the available actions are calculated. Let's show an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[board] Placed Albert - Warrior at position Position(x=0, y=0)\n",
      "[board] Placed Exit portal at position Position(x=2, y=2)\n",
      "[board] Placed Archer 1 at position Position(x=1, y=2)\n",
      "[board] Placed Swordsman 1 at position Position(x=1, y=1)\n",
      "[board] Placed Apprentice 1 at position Position(x=1, y=0)\n",
      "[board] Placed Archer 2 at position Position(x=0, y=1)\n",
      "======================\n",
      "|      | A(2) | EXIT |\n",
      "----------------------\n",
      "| A(1) | S(2) |      |\n",
      "----------------------\n",
      "| H    | M(1) |      |\n",
      "----------------------\n",
      "======================\n",
      "Available actions:\n",
      "[board] Positions with enemies in attack range: [Position(x=0, y=1), Position(x=1, y=0)]\n",
      "['Move up', 'Move right', 'Attack']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "from game_client import GameClient\n",
    "\n",
    "# Hero classes: Warrior, Rogue, Mage\n",
    "from questing import Warrior, Rogue, Mage\n",
    "from questing import Position\n",
    "hero = Warrior('Albert')\n",
    "board_width = 3\n",
    "board_height = 3\n",
    "num_enemies = 4\n",
    "\n",
    "game = GameClient(hero=hero, board_width=board_width, board_height=board_height, num_enemies=num_enemies)\n",
    "# To start, this will just show the board and the available actions\n",
    "game.play()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the available actions include \"Move up\" and \"Move right\", but if you try to perform one of these, you'll get an error like this (and no movement  will happen):\n",
    "\n",
    "```\n",
    "[board] Can't move to position Position(x=0, y=1), it's not empty!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taking action Move up\n",
      "[board] Positions with enemies in attack range: [Position(x=0, y=1), Position(x=1, y=0)]\n",
      "[board] Can't move to position Position(x=0, y=1), it's not empty!\n",
      "======================\n",
      "|      | A(2) | EXIT |\n",
      "----------------------\n",
      "| A(1) | S(2) |      |\n",
      "----------------------\n",
      "| H    | M(1) |      |\n",
      "----------------------\n",
      "======================\n",
      "Available actions:\n",
      "[board] Positions with enemies in attack range: [Position(x=0, y=1), Position(x=1, y=0)]\n",
      "['Move up', 'Move right', 'Attack']\n"
     ]
    }
   ],
   "source": [
    "game.play(\"Move up\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "You are now responsible for fixing this bug! We'll just guide you through the high-level steps, trying to follow TDD.\n",
    "\n",
    "### RED - Add a failing test\n",
    "\n",
    "First thing to do is to add a failing test to the file ```tests/game_test.py```. What do we want to test? The high-level steps would be as follows:\n",
    "\n",
    "- The context: the \"up\" position is valid (i.e. ```board.is_valid``` should return True) but not empty (i.e. ```board.is_empty``` should return False.\n",
    "\n",
    "- The expected result: ```GameActions.MOVE_UP```` shouldn't be one of the available actions.\n",
    "\n",
    "The test ```test_available_actions_with_move_up``` is a good starting point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "The test could look something like this:\n",
    "    \n",
    "```python\n",
    "def test_available_actions_occupied_slot(game, board):\n",
    "    up_position = Position(0, 1)\n",
    "\n",
    "    board.is_valid.return_value = True\n",
    "    board.is_empty.return_value = False\n",
    "\n",
    "    available_actions = game.available_actions()\n",
    "\n",
    "    assert GameActions.MOVE_UP not in available_actions, f\"GameActions.MOVE_UP should not be in the available actions\"\n",
    "    board.is_valid.assert_any_call(up_position)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "### GREEN - Make any changes needed to make the test pass\n",
    "\n",
    "Next, you will have to make any changes needed in the ```Game.available_actions``` method to make the test pass. HINT: The test is failing because the ```Game.available_actions``` is not taking into account whether the destination position is empty or not, it only checks whether the position is valid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "In the ```available_actions``` method, you will need to change lines like this:\n",
    "\n",
    "```python\n",
    "    if self.board.is_valid(up):\n",
    "        available_actions.append(GameActions.MOVE_UP)\n",
    "\n",
    "    if self.board.is_valid(down):\n",
    "        available_actions.append(GameActions.MOVE_DOWN)\n",
    "```\n",
    "\n",
    "to this:\n",
    "\n",
    "```python\n",
    "    if self.board.is_valid(up) and self.board.is_empty(up):\n",
    "        available_actions.append(GameActions.MOVE_UP)\n",
    "\n",
    "    if self.board.is_valid(down) and self.board.is_empty(down):\n",
    "        available_actions.append(GameActions.MOVE_DOWN)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REFACTOR - Consolidate changes\n",
    "\n",
    "There's probably no need for a refactor in this case, but feel free to make any refactors you see fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
