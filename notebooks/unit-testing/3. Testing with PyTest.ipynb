{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with PyTest\n",
    "\n",
    "For the next steps, make sure that you open a terminal and that you activate the virtual environment.\n",
    "\n",
    "## Running your first test\n",
    "\n",
    "For most of the examples and exercises from here, we will need to use a terminal. Also, the different examples and files we'll be working on are located inside the \"testing_exercises\" folder in the root of this project.\n",
    "\n",
    "For the first example, we have put the ```add_two``` function and the three tests we have into a single file, inside the \"01_intro/first_test.py\". The code in that file looks like this:\n",
    "\n",
    "```python\n",
    "def add_two(number: int) -> int:\n",
    "    \"\"\"This function just adds 2 to any number it receives\"\"\"\n",
    "    return number + 2\n",
    "\n",
    "def test_add_two():\n",
    "    number = 1\n",
    "    expected_result = 3\n",
    "\n",
    "    result = add_two(number)\n",
    "\n",
    "    assert result == expected_result, \\\n",
    "        f\"The result of adding 2 to {number} should be {expected_result}, but it was {result}!\"\n",
    "\n",
    "def test_add_two_to_3():\n",
    "    number = 3\n",
    "    expected_result = 5\n",
    "\n",
    "    result = add_two(number)\n",
    "\n",
    "    assert result == expected_result, \\\n",
    "        f\"The result of adding 2 to {number} should be {expected_result}, but it was {result}!\"\n",
    "\n",
    "def test_add_two_to_minus_6():\n",
    "    number = -6\n",
    "    expected_result = -4\n",
    "\n",
    "    result = add_two(number)\n",
    "\n",
    "    assert result == expected_result, \\\n",
    "        f\"The result of adding 2 to {number} should be {expected_result}, but it was {result}!\"\n",
    "```\n",
    "\n",
    "Now, assuming that you are inside the \"testing_exercises\" folder in the terminal, you can run the tests like this:\n",
    "\n",
    "```bash\n",
    "pytest 01_intro/first_test.py\n",
    "```\n",
    "\n",
    "And its output should look more or less like this:\n",
    "\n",
    "```bash\n",
    "=============================== test session starts ===============================\n",
    "platform win32 -- Python 3.7.3, pytest-5.1.2, py-1.8.0, pluggy-0.12.0\n",
    "rootdir: C:\\...\\software-engineering-workshop\\testing_exercises\n",
    "plugins: pyfakefs-3.6, cov-2.7.1\n",
    "collected 3 items               \n",
    "\n",
    "01_intro\\first_test.py ...                                                   [100%]\n",
    "\n",
    "================================ 3 passed in 0.03s ================================\n",
    "```\n",
    "\n",
    "You can also pass a full directory with test files, and by default, pytest will look for all available tests inside the current directory (or any of its subdirectories), meaning that if you had run ```pytest 01_intro``` the result would have been the same.\n",
    "\n",
    "With pytest, you can write your tests inside a class, so that they are logically grouped together, or you can just have them inside the same file. Each has its pros and cons. For the class, it gives room for reusing other libraries such as the unittest, which provides a TestCase class where you can easily have your setUp and tearDown methods, but on the other hand, you might have issues when using things like parametrized tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running with coverage reports\n",
    "\n",
    "Pytest has lots of options, but one you might definitely want is the coverage report. With this, after running your tests you will get a report with information about how much of your code is covered by unit tests, and for each file in your project, what lines are not covered yet.\n",
    "\n",
    "The coverage is actually a plugin, which is installed as a separate package: [```pytest-cov```](https://pypi.org/project/pytest-cov/). Here's an generic command to run tests with coverage:\n",
    "\n",
    "```bash\n",
    "pytest --cov-report term-missing --cov=[YOUR_PACKAGE_FOLDER] [YOUR_TESTS_FOLDER]\n",
    "```\n",
    "\n",
    "So again, assuming that we are in the project root folder and that we have this folder structure:\n",
    "\n",
    "```\n",
    "/[PROJECT BASE DIR]\n",
    "│\n",
    "├── setup.py\n",
    "├── /mypackage\n",
    "│   ├── __init__.py\n",
    "│   ├── my_class.py\n",
    "│   ├── ...\n",
    "│   ├── another_class.py\n",
    "│   └── /mysubmodule\n",
    "│       ├── __init__.py\n",
    "│       └── subclass.py\n",
    "└── /tests\n",
    "    ├── my_class_test.py\n",
    "    ├── ...\n",
    "    ├── another_class_test.py\n",
    "    └── /mysubmodule\n",
    "        └── subclass_test.py\n",
    "```\n",
    "\n",
    "We can run the tests with coverage for ```mypackage``` like this:\n",
    "\n",
    "```bash\n",
    "pytest --cov-report term-missing --cov=mypackage tests\n",
    "```\n",
    "\n",
    "And the output will look somewhat like this (note that this output is for another project):\n",
    "\n",
    "```bash\n",
    "=============================== test session starts ===============================\n",
    "platform win32 -- Python 3.7.3, pytest-5.1.2, py-1.8.0, pluggy-0.12.0\n",
    "rootdir: C:\\Users\\kr99tud\\projects\\ml-engineer\\se-project\n",
    "plugins: pyfakefs-3.6, cov-2.7.1\n",
    "collected 5 items                                                                  \n",
    "\n",
    "tests\\logger_test.py ..                                                      [ 40%]\n",
    "tests\\types_test.py ...                                                      [100%]\n",
    "\n",
    "----------- coverage: platform win32, python 3.7.3-final-0 -----------\n",
    "Name                       Stmts   Miss  Cover   Missing\n",
    "--------------------------------------------------------\n",
    "questing\\__init__.py           7      0   100%\n",
    "questing\\attack.py            40     23    42%   8-9, 13, 17-24, 28-29, 32-46, 50-51, 56\n",
    "questing\\board.py             94     71    24%   14-19, 23, 27-31, 34, 37-52, 55-66, 69-77, 80, 83, 87-104, 107-114, 118-133\n",
    "questing\\defense.py           47     31    34%   10-12, 15-23, 26-33, 37, 41-42, 45-47, 50-53, 57-67\n",
    "questing\\enemy.py            136     69    49%   14-15, 18, 22-26, 38-45, 49, 60-67, 71, 83-91, 95, 99-103, 107, 111-114, 118, 122, 126, 130, 134, 138, 141-143, 146-155, 158-163, 166-173, 177, 181\n",
    "questing\\game.py              95     73    23%   26-29, 32-68, 71-108, 111-121\n",
    "questing\\game_element.py      21      6    71%   10-11, 16, 20, 24, 28\n",
    "questing\\hero.py              20      7    65%   11, 15-16, 20-21, 25-26\n",
    "questing\\logger.py             7      0   100%\n",
    "questing\\types.py              6      1    83%   9\n",
    "questing\\unit.py              26     15    42%   9-10, 13-25, 28, 32, 36\n",
    "--------------------------------------------------------\n",
    "TOTAL                        499    296    41%\n",
    "\n",
    "\n",
    "================================ 5 passed in 0.45s ================================\n",
    "```\n",
    "\n",
    "The table you see here has one entry per file, and for every file, you get a row like this:\n",
    "\n",
    "```bash\n",
    "----------- coverage: platform win32, python 3.7.3-final-0 -----------\n",
    "Name                       Stmts   Miss  Cover   Missing\n",
    "--------------------------------------------------------\n",
    "questing\\unit.py              26     15    42%   9-10, 13-25, 28, 32, 36\n",
    "```\n",
    "\n",
    "This means that the file ```questing/attack.py``` contains 40 statements (1 statement $\\approx$ 1 line), that your tests *missed* 23 of these lines, so they only covered 17 statements.\n",
    "\n",
    "The total coverage is 42% (calculated as ```17 / 40```), and it also tells you what lines were not tested, e.g. lines 14 to 19, line 23, lines 27 to 31...\n",
    "\n",
    "**IMPORTANT things to keep in mind**:\n",
    "\n",
    "- The coverage includes lines reached by ANY test, not only by the ones specific to the file. in the case of the ```questing/attack.py```, this means that the coverage is not considering only the lines covered by ```tests/attack_test.py```, but also by any other files that might be using ```questing/attack.py```.\n",
    "- As a consequence of the previous point, if you run the tests for a single file (e.g. ```pytest [...] tests/attack_test.py```, the coverage might be lower for the ```attack.py``` file, and will definitely be lower for the rest of files.\n",
    "- Ideally: your test file for ```attack.py``` should result in a 100% coverage of the file, this way you don't depend on what other files do.\n",
    "\n",
    "### Saving coverage reports to HTML / XML format\n",
    "\n",
    "One last interesting option is to generate a version of the coverage report in HTML (usually for human readers) or XML (usually for machines / other programs to read). For this, you can use the options ```--cov-report \"html:[REPORT_DIR]\"``` and ```--cov-report \"xml:[REPORT_FILE]\"``` respectively, for instance, in the case of the ```questing``` package:\n",
    "\n",
    "```bash\n",
    "pytest \\\n",
    "    --cov-report \"html:.cov_reports/html/questing\" \\\n",
    "    --cov-report \"xml:.cov_reports/xml/questing.xml\" \\\n",
    "    --cov-report term-missing \\\n",
    "    --cov=questing \\\n",
    "    tests\n",
    "```\n",
    "\n",
    "For **Windows users**, the previous command would be:\n",
    "\n",
    "```bash\n",
    "pytest ^\n",
    "    --cov-report \"html:.cov_reports/html/questing\" ^\n",
    "    --cov-report \"xml:.cov_reports/xml/questing.xml\" ^\n",
    "    --cov-report term-missing ^\n",
    "    --cov=questing ^\n",
    "    tests\n",
    "```\n",
    "\n",
    "This will yield the same output as before, but it will also create an html version of the coverage report, and an xml version of it, inside the ```.cov_reports``` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parametrized tests\n",
    "\n",
    "Consider this class:\n",
    "\n",
    "```python\n",
    "from typing import Union\n",
    "\n",
    "Number = Union[int, float]\n",
    "class Calculator:\n",
    "    def sum(self, x: Number, y: Number) -> Number:\n",
    "        return x + y\n",
    "```\n",
    "\n",
    "And a few tests we have created for it:\n",
    "\n",
    "```python\n",
    "def test_sum_1_and_2():\n",
    "    x = 1\n",
    "    y = 2\n",
    "    expected_result = 3\n",
    "    \n",
    "    result = Calculator().sum(x, y)\n",
    "\n",
    "    assert result == expected_result, \\\n",
    "        f\"{x} + {y} is {expected_result}, but the actual result was {result}\"\n",
    "\n",
    "def test_sum_3_and_0():\n",
    "    x = 3\n",
    "    y = 0\n",
    "    expected_result = 3\n",
    "    \n",
    "    result = Calculator().sum(x, y)\n",
    "\n",
    "    assert result == expected_result, \\\n",
    "        f\"{x} + {y} is {expected_result}, but the actual result was {result}\"\n",
    "\n",
    "def test_sum_minus_5_and_2():\n",
    "    # ...\n",
    "\n",
    "def test_sum_minus_5_and_minus_2():\n",
    "    # ...\n",
    "```\n",
    "\n",
    "If you think of it, the only difference between every test is just the values of ```x```, ```y``` and the ```expected_result```, so it would be handy to have a function to which we can pass these parameters and that would run the test. A first implementation of that could look like this:\n",
    "\n",
    "```python\n",
    "def test_sums():\n",
    "    sum_data = [\n",
    "        #x, y, expected_result\n",
    "        (1, 2, 3),\n",
    "        (3, 0, 3),\n",
    "        (-5, 2, -3),\n",
    "        (-5, -2, -7),\n",
    "    ]\n",
    "    \n",
    "    for x, y, expected_result in sum_data:\n",
    "        result = Calculator().sum(x, y)\n",
    "\n",
    "        assert result == expected_result, \\\n",
    "            f\"{x} + {y} is {expected_result}, but the actual result was {result}\"\n",
    "```\n",
    "\n",
    "You can run this example with:\n",
    "\n",
    "```bash\n",
    "pytest 02_parametrized_test/single_test.py\n",
    "```\n",
    "\n",
    "This approach would work, but has some issues:\n",
    "\n",
    "- Everything runs as a single test (i.e. you don't have one test for each of the input rows).\n",
    "- As a consequence, if one of the assertion fails (or one of the calls throws an exception, etc), we might not be testing everything.\n",
    "\n",
    "That's where parametrized tests come in. It will look similar to our ```test_sums``` function, but we factor the for loop out of it. Pytest provides this feature, so our previous test can be rewritten like this:\n",
    "\n",
    "```python\n",
    "import pytest\n",
    "\n",
    "SUM_FIXTURES = [\n",
    "    (1, 2, 3),\n",
    "    (1, 5, 6),\n",
    "    (-5, 4, -1),\n",
    "]\n",
    "\n",
    "@pytest.mark.parametrize(\"x,y,expected_result\", SUM_FIXTURES)\n",
    "def test_sum(x, y, expected_result):\n",
    "    result = Calculator().sum(x, y)\n",
    "\n",
    "    assert result == expected_result, \\\n",
    "        f\"{x} + {y} is {expected_result}, but the actual result was {result}\"\n",
    "```\n",
    "\n",
    "Again, you can run this example with:\n",
    "\n",
    "```bash\n",
    "pytest 02_parametrized_test/parametrized_test.py\n",
    "```\n",
    "\n",
    "The main benefit of this is that in our output, we get one test for each element inside ```SUM_FIXTURES```, so if one of the cases there fails, the rest are of cases are still test and marked as succeed / fail accordingly. Let's say we add ```(1, 4, 6)``` to our ```SUM_FIXTURES```. The test for that case will fail miserably, but the rest will still succeed:\n",
    "\n",
    "```bash\n",
    "=============================== test session starts ===============================\n",
    "platform win32 -- Python 3.7.3, pytest-5.1.2, py-1.8.0, pluggy-0.12.0\n",
    "rootdir: C:\\...\\software-engineering-workshop\\testing_exercises\n",
    "plugins: pyfakefs-3.6, cov-2.7.1\n",
    "collected 4 items\n",
    "\n",
    "02_parametrized_test\\parametrized_test.py ..F.                                 [100%]\n",
    "\n",
    "==================================== FAILURES =====================================\n",
    "_________________________________ test_sum[1-4-6] _________________________________\n",
    "\n",
    "x = 1, y = 4, expected_result = 6\n",
    "\n",
    "    @pytest.mark.parametrize(\"x,y,expected_result\", SUM_FIXTURES)\n",
    "    def test_sum(x, y, expected_result):\n",
    "        result = Calculator().sum(x, y)\n",
    "    \n",
    ">       assert result == expected_result, \\\n",
    "            f\"{x} + {y} is {expected_result}, but the actual result was {result}\"\n",
    "E       AssertionError: 1 + 4 is 6, but the actual result was 5\n",
    "E       assert 5 == 6\n",
    "\n",
    "02_parametrized_test\\parametrized_test.py:21: AssertionError\n",
    "=========================== 1 failed, 3 passed in 0.14s ===========================\n",
    "```\n",
    "\n",
    "If we had done it in just one test method instead, we would get 1 failure and 0 passed, and we wouldn't be sure of how many of the test cases are failing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixtures and mocks\n",
    "\n",
    "In most real applications, your classes and functions will have dependencies. One of the key parts to keep your tests maintainable is isolation, meaning that if you have class B, that depends on class A, your tests for B should not depend on the real behavior of A. To do that, we will be introducing Mock objects, as well as another important feature of pytest: test fixtures.\n",
    "\n",
    "## Scenario\n",
    "\n",
    "To set the context, consider the two classes below:\n",
    "\n",
    "```python\n",
    "class TaxCalculator:\n",
    "    def __init__(self, tva: float = .21):\n",
    "        self.tva = tva\n",
    "\n",
    "    def add_taxes(self, amount: float) -> float:\n",
    "        return amount * (1 + self.tva)\n",
    "\n",
    "class Bill:\n",
    "    def __init__(self, tax_calculator: \"TaxCalculator\"):\n",
    "        self.tax_calculator = tax_calculator\n",
    "        self.amount: float = 0.\n",
    "\n",
    "    def add(self, amount: float):\n",
    "        self.amount += amount\n",
    "    \n",
    "    @property\n",
    "    def total(self) -> float:\n",
    "        \"\"\"Calculates the total amount, including taxes\"\"\"\n",
    "        return self.tax_calculator.add_taxes(self.amount)\n",
    "```\n",
    "\n",
    "When testing it, you might think of doing something like this:\n",
    "\n",
    "```python\n",
    "def test_total():\n",
    "    tax_calculator = TaxCalculator(tva=.1)\n",
    "    bill = Bill(tax_calculator=tax_calculator)\n",
    "    bill.add(100.0)\n",
    "    expected_total = 110.0\n",
    "    \n",
    "    total = bill.total()\n",
    "    \n",
    "    assert total == expected_total, f\"Total was {total}, but it should be {expected_total}\"\n",
    "```\n",
    "\n",
    "While this might work, this approach introduces a few problems:\n",
    "- Our tests are not isolated anymore: if the TaxCalculator implementation changes, our tests might start breaking. When testing the Bill class, we don't want to depend on the logic implemented inside the TaxCalculator class.\n",
    "- We will have to create the ```TaxCalculator``` for every test where we want to use it. This means that if the way we create a TaxCalculator object changes, we'll have to change all our tests.\n",
    "\n",
    "To solve the first problem, we'll be using Mock objects: objects that mimic the behaviour of other objects, but that have no actual logic in them.\n",
    "\n",
    "To solve the second problem, we'll use fixtures.\n",
    "\n",
    "Again, you can run this example yourself. If you're inside the ```testing_exercises``` folder, just run:\n",
    "\n",
    "```bash\n",
    "pytest 03_fixtures/01_bill_calculator.py\n",
    "\n",
    "=========================== test session starts ===========================\n",
    "platform win32 -- Python 3.7.3, pytest-5.1.2, py-1.8.0, pluggy-0.12.0\n",
    "rootdir: C:\\...\\software-engineering-workshop\\testing_exercises\n",
    "plugins: pyfakefs-3.6, cov-2.7.1\n",
    "collected 1 item                                                           \n",
    "\n",
    "03_fixtures\\01_bill_calculator.py .                                  [100%]\n",
    "\n",
    "============================ 1 passed in 0.05s ============================\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mocks\n",
    "\n",
    "As we just explained, in order to keep our tests for the ```Bill``` class isolated, we need a way to not depend on the ```TaxCalculator``` class. We can do that by using the [```unittest.mock```](https://docs.python.org/3/library/unittest.mock.html) library. With it, you can create objects that mimic other objects and/or functions.\n",
    "\n",
    "To create a mock, there are two main classes: ```Mock``` and ```MagicMock```. The difference between the two is that ```MagicMock``` implements Python's magic methods, such as ```__str__```.\n",
    "\n",
    "### Creating a mock\n",
    "\n",
    "To create a mock, we just need to instantiate the MagicMock class:\n",
    "\n",
    "```python\n",
    "from unittest import mock\n",
    "\n",
    "tax_calculator = mock.MagicMock()\n",
    "```\n",
    "\n",
    "Now that we have a mock, we can arbitrarily decide what calling a function will return. We'll see different ways of defining this return values.\n",
    "\n",
    "### Return a constant value\n",
    "\n",
    "The simplest way is to have a method always return the same value:\n",
    "\n",
    "```python\n",
    "tax_calculator.add_taxes.return_value = 110.0\n",
    "```\n",
    "\n",
    "With this, anytime the ```add_taxes``` method of our mock is called, ```110.0``` will be returned, no matter the arguments passed to the function.\n",
    "\n",
    "### Returning different values in successive calls\n",
    "\n",
    "Sometimes, you want the method to return value \"x\" on the first call, \"y\" on the second call, etc. For this, we can use the ```side_effect``` property, passing a list of values to it: \n",
    "\n",
    "```python\n",
    "tax_calculator.add_taxes.side_effect = [100.0, 200.0, 50.0]\n",
    "```\n",
    "\n",
    "This will return 100.0 on the first call to ```tax_calculator.add_taxes```, 200.0 on the second one and 50.0 on the third one. **IMPORTANT**: If the method is called a fourth time, an exception will be raised!\n",
    "\n",
    "### Calculating the return value dynamically\n",
    "\n",
    "Finally, you can also decide the return value using your own function, and passing it to the side_effect:\n",
    "\n",
    "```python\n",
    "def add_taxes_side_effect(amount: float):\n",
    "    return amount + 10\n",
    "\n",
    "tax_calculator.add_taxes.side_effect = add_taxes_side_effect\n",
    "```\n",
    "\n",
    "### Raising errors from mocks\n",
    "\n",
    "Another thing you can do is to have your mock raise a certain error when it is called. For this, you can also use the side_effect property:\n",
    "\n",
    "```python\n",
    "error = ValueError(\"Invalid amount\")\n",
    "tax_calculator.add_taxes.side_effect = error\n",
    "```\n",
    "\n",
    "### Asserting calls to a mock method\n",
    "\n",
    "Finally, it is a common practice in unit tests to assert that the mock methods you expect to be called have actually been called, and that they have been called with the right values. There is a whole range of methods to make different assertions, for instance: ```assert_called``` to just check that it has been called, no matter with what parameters, ```assert_called_once``` to assert that it was called once and only once, ```assert_any_call``` to assert that at least one of the calls to the mock has been made with the arguments we pass, etc. You can find the documentation [here](https://docs.python.org/3/library/unittest.mock.html#unittest.mock.Mock.assert_called)\n",
    "\n",
    "### Rewriting our test with mocks\n",
    "\n",
    "Now that we have see how to create mocks, we can rewrite our test like this:\n",
    "\n",
    "```python\n",
    "import pytest\n",
    "from unittest import mock\n",
    "\n",
    "def test_total():\n",
    "    tax_calculator = mock.MagicMock()\n",
    "    bill = Bill(tax_calculator=tax_calculator)\n",
    "    amount_to_add = 100.0\n",
    "    amount_after_taxes = 110.0\n",
    "    bill.add(amount_to_add)\n",
    "    tax_calculator.add_taxes.return_value = amount_after_taxes\n",
    "    \n",
    "    total = bill.total\n",
    "\n",
    "    assert total == amount_after_taxes, f\"Total was {total}, but it should be {expected_total}\"\n",
    "    tax_calculator.add_taxes.assert_called_once_with(amount_to_add)\n",
    "```\n",
    "\n",
    "You can run the test with:\n",
    "\n",
    "```bash\n",
    "pytest 03_fixtures/02_bill_calculator_magic_mock.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixtures\n",
    "\n",
    "The last piece we'll introduce now are fixtures. Fixtures allow you to inject dependencies so that you can have full control over them. They include not only mocks but any other thing you might want to reuse across tests. You can find the full documentation for fixtures in the [pytest docs](https://docs.pytest.org/en/latest/fixture.html)\n",
    "\n",
    "### Defining fixtures\n",
    "\n",
    "You can create a fixture with the ```pytest.fixture``` decorator:\n",
    "\n",
    "```python\n",
    "import pytest\n",
    "\n",
    "@pytest.fixture\n",
    "def tax_calculator():\n",
    "    tax_calculator = mock.MagicMock()\n",
    "    \n",
    "    return tax_calculator\n",
    "```\n",
    "\n",
    "### Using fixtures\n",
    "\n",
    "To use fixtures in a test, you simply need to add an argument to your test with the fixture name:\n",
    "\n",
    "```python\n",
    "def my_test(tax_calculator):\n",
    "    # do something\n",
    "```\n",
    "\n",
    "### Available fixtures\n",
    "\n",
    "Pytest already has some [built-in fixtures](https://pytest.readthedocs.io/en/latest/builtin.html#builtin-fixtures-function-arguments) (e.g. to capture logs, the standard output..), and the pyfakefs library we have also adds a ```fs``` fixture that you can use in your tests.\n",
    "\n",
    "If you want to check what fixtures are available for all tests, you can run this:\n",
    "\n",
    "```python\n",
    "pytest --fixtures\n",
    "```\n",
    "\n",
    "Now, some files might define their own fixtures. You can check the fixtures available for a specific file by also providing the name of the file:\n",
    "\n",
    "```python\n",
    "pytest --fixtures my_file.py\n",
    "```\n",
    "\n",
    "### Sharing fixtures across multiple tests\n",
    "\n",
    "Sometimes, you don't want to define a fixture only for one test file, but you want one that you can reuse through any test. To do that, you can create a ```conftest.py``` file with the fixture, and any test that's on the same directory (or in a subdirectory) will be able to use the fixture.\n",
    "\n",
    "### Fixture scope\n",
    "\n",
    "A detail that can be important when writing tests is the scope of fixtures. By default, every test case \n",
    "using the fixture will instantiate it again (i.e. the fixture function will be called again). This is ok in many cases, but it's probably not what you want if your fixtures take a long time to be created (e.g. if you want to create a spark session); this is where the scope of fixtures comes into play. You can define the scope of your fixture by passing it to the decorator:\n",
    "\n",
    "```python\n",
    "import pytest\n",
    "from unittest import mock\n",
    "\n",
    "@pytest.fixture(scope=\"session\")\n",
    "def my_session_fixture():\n",
    "    return mock.MagicMock()\n",
    "\n",
    "@pytest.fixture(scope=\"class\")\n",
    "def my_class_fixture():\n",
    "    return mock.MagicMock()\n",
    "```\n",
    "\n",
    "The possible values for ```scope``` are:\n",
    "\n",
    "- **function** (default value): You fixture will be instantiated everytime it's used.\n",
    "- **class**: The fixture is only instantiated once per class. So, if you have classes \"ATest\" and \"BTest\", your fixture will be instantiated twice.\n",
    "- **module**: The fixture is instantiated once per module (i.e. per subdirectory)\n",
    "- **package**: In this case, the fixture is instantiated once per package. Note that a package can have multiple modules.\n",
    "- **session**: This is the \"broadest\" level. The fixture will just be instantiated once.\n",
    "\n",
    "More information:\n",
    "https://docs.pytest.org/en/latest/fixture.html#scope-sharing-a-fixture-instance-across-tests-in-a-class-module-or-session\n",
    "\n",
    "### Adding a tax_calculator fixture\n",
    "\n",
    "Finally, let's add a tax_calculator fixture to our test.\n",
    "\n",
    "You can run the example with:\n",
    "\n",
    "```bash\n",
    "pytest 03_fixtures/03_bill_calculator_fixtures.py --capture=no\n",
    "```\n",
    "\n",
    "There are two tests using the fixture, so you should see twice the line \"Instantiating tax_calculator fixture\". There is another example that uses the ```module``` scope, that you can also run:\n",
    "\n",
    "```bash\n",
    "pytest 03_fixtures/04_scoped_fixtures.py --capture=no\n",
    "```\n",
    "\n",
    "**IMPORTANT**: Be careful with the scope of the fixtures and the assertions. For instance, if you have a fixture with scope \"module\" and an assertion of the type \"my_fixture.some_method.assert_called_once()\" in more that one test, your assertions might fail, since the call counts for the mock will not be reset between tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other tips\n",
    "\n",
    "## Testing exceptions\n",
    "\n",
    "Sometimes, your test will expect an exception to be thrown. You could manually have the \"act\" part of your test wrapped in a try-catch, but pytest also provides a way to expect a certain type of exception to be raised, [```pytest.raises```](http://doc.pytest.org/en/latest/assert.html#assertions-about-expected-exceptions):\n",
    "\n",
    "```python\n",
    "import pytest\n",
    "\n",
    "def my_test():\n",
    "    with pytest.raises(ZeroDivisionError):\n",
    "        1 / 0\n",
    "```\n",
    "\n",
    "If you also want access to the error message, you can get it too:\n",
    "\n",
    "```python\n",
    "def test_value_error():\n",
    "    with pytest.raises(ValueError) as excinfo:\n",
    "        def f():\n",
    "            raise ValueError(\"Some error\")\n",
    "        f()\n",
    "    assert \"Some error\" == str(excinfo.value)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
