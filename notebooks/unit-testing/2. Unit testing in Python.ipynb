{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit Testing in Python\n",
    "\n",
    "In this notebook, we'll introduce pytest and we'll go one step further with TDD, creating tests suites (i.e. groups of tests) and some more advanced features.\n",
    "\n",
    "We will be using [pytest](https://docs.pytest.org/en/latest/) together with [unittest](https://docs.python.org/3/library/unittest.html), since they combine nicely for some parts. But before, there are some extra concepts that we need to introduce and that we haven't explained yet.\n",
    "\n",
    "## Basic concepts and terminology\n",
    "\n",
    "### Mocking\n",
    "\n",
    "In the [previous notebook](1.%20Introduction_to_testing.ipynb#Unit-testing), we explained that one of the basic requirements for a unit test is isolation. However, in practice our classes might depend on other classes, meaning that our test is somehow \"coupled\" to the actual implementation of the other class.\n",
    "\n",
    "This is what the concept of *mocking* tries to fix. To keep it short, mocking is about creating objects that simulate the real objects. More info [here](https://stackoverflow.com/questions/2665812/what-is-mocking).\n",
    "\n",
    "In pure terms, there are different types of these \"fake\" objects:\n",
    "\n",
    "- A **Stub** is just an object that has the same methods as the real one, but does nothing at all when you call them.\n",
    "- A **Spy** is like a stub, but keeps track of what methods where called, how many times, maybe also with what parameters, etc..\n",
    "- A **Mock**, which can be seen as a spy that also allows to define its behavior, meaning that we can decide what value will be returned by a function (and/or under what circumstances). This is what we'll be using in our tests.\n",
    "\n",
    "### Test suite\n",
    "\n",
    "Very often, tests are grouped together in what's called a **test suite**. For instance, when testing a class, you probably want to test each method indivually, but since all tests relate to the same class, it still makes sense to keep them together in one place. This is what a test suite is about, and in practice, most of the times you will have one test suite per class.\n",
    "\n",
    "### Other terms\n",
    "\n",
    "- **setUp** / **tearDown**: Most frameworks implement some special methods that allow you to setup your context BEFORE every test runs, and to clean up AFTER every test runs. Often these are called setUp / tearDown, although in other libraries or languages they might have different names (e.g. \"before\" / \"after\").\n",
    "\n",
    "- **assertion**: An assertion is just a check. Most frameworks ofter shortcuts for different assertions, e.g. \"assertTrue(x)\" would be equivalent to \"assert x == True\".\n",
    "\n",
    "- **fixture**: We'll see more on this later, but you can see a fixture as some sort of dependency that you want to reuse across tests. This could be a mock, or a real connection to a database, a file, etc... Sometimes you might also read / hear about \"data fixtures\", which basically refers to data used for testing purposes.\n",
    "\n",
    "- **(code) coverage**: This is just a ratio that indicates which % of your source code is covered by tests. It's important to note that a 100% coverage doesn't mean you're covering all possible test cases, just that the tests you have are \"passing\" at least once over every single line of code in your project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries\n",
    "\n",
    "As mentioned, we'll be using the [```unittest``` module](https://docs.python.org/3/library/unittest.html) from Python, and [PyTest](https://docs.pytest.org/en/latest/), which simplifies some routine tasks like creating test suites or running your tests, among other features.\n",
    "\n",
    "Other than these two, we'll also be using [```pyfakefs```](https://pypi.org/project/pyfakefs/), which allows us to test using a fake file system, and [```pytest-cov```](https://pypi.org/project/pytest-cov/), which is a pytest plugin providing coverage reports.\n",
    "\n",
    "## Structuring your tests\n",
    "\n",
    "In most Python packages, you will have a structure that somewhat ressembles this:\n",
    "\n",
    "```\n",
    "/[PROJECT BASE DIR]\n",
    "│\n",
    "├── setup.py\n",
    "└── /mypackage\n",
    "    ├── __init__.py\n",
    "    ├── my_class.py\n",
    "    ├── ...\n",
    "    ├── another_class.py\n",
    "    └── /mysubmodule\n",
    "        └── subclass.py\n",
    "```\n",
    "\n",
    "In general, you want to have your tests in the same project, but not in the same \"package\", so most of the times, you will just see a \"tests\" folder in the project dir. It is not enforced, but usually the convention is to follow the same structure as the package, having one file per file in the package, just suffixed (or prefixed) with **test**. So, the previous project structure could look like this with tests:\n",
    "\n",
    "```\n",
    "/[PROJECT BASE DIR]\n",
    "│\n",
    "├── setup.py\n",
    "├── /mypackage\n",
    "│   ├── __init__.py\n",
    "│   ├── my_class.py\n",
    "│   ├── ...\n",
    "│   ├── another_class.py\n",
    "│   └── /mysubmodule\n",
    "│       ├── __init__.py\n",
    "│       └── subclass.py\n",
    "└── /tests\n",
    "    ├── my_class_test.py\n",
    "    ├── ...\n",
    "    ├── another_class_test.py\n",
    "    └── /mysubmodule\n",
    "        └── subclass_test.py\n",
    "```\n",
    "\n",
    "Now, inside each of the ```*_test.py``` files, you will have a test suite (a group of tests). Here you also have different options: you can have one function per test, which makes it easy to use some extra features in pytest, but can make the preparation for each test a bit harder (e.g. if you need to create fake data, fake objects, etc), or you can have one class per test, which makes the preparation for each test easier, but adds limitations in some features (e.g. parametrized tests). So, the style you use is more of a personal choice, and you can actually mix both in a project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with PyTest\n",
    "\n",
    "For the next steps, make sure that you open a terminal and that you activate the virtual environment.\n",
    "\n",
    "## Running your first test\n",
    "\n",
    "For most of the examples and exercises from here, we will need to use a terminal. Also, the different examples and files we'll be working on are located inside the \"testing_exercises\" folder in the root of this project.\n",
    "\n",
    "For the first example, we have put the ```add_two``` function and the three tests we have into a single file, inside the \"01_intro/first_test.py\". The code in that file looks like this:\n",
    "\n",
    "```python\n",
    "def add_two(number: int) -> int:\n",
    "    \"\"\"This function just adds 2 to any number it receives\"\"\"\n",
    "    return number + 2\n",
    "\n",
    "def test_add_two():\n",
    "    number = 1\n",
    "    expected_result = 3\n",
    "\n",
    "    result = add_two(number)\n",
    "\n",
    "    assert result == expected_result, \\\n",
    "        f\"The result of adding 2 to {number} should be {expected_result}, but it was {result}!\"\n",
    "\n",
    "def test_add_two_to_3():\n",
    "    number = 3\n",
    "    expected_result = 5\n",
    "\n",
    "    result = add_two(number)\n",
    "\n",
    "    assert result == expected_result, \\\n",
    "        f\"The result of adding 2 to {number} should be {expected_result}, but it was {result}!\"\n",
    "\n",
    "def test_add_two_to_minus_6():\n",
    "    number = -6\n",
    "    expected_result = -4\n",
    "\n",
    "    result = add_two(number)\n",
    "\n",
    "    assert result == expected_result, \\\n",
    "        f\"The result of adding 2 to {number} should be {expected_result}, but it was {result}!\"\n",
    "```\n",
    "\n",
    "Now, assuming that you are inside the \"testing_exercises\" folder in the terminal, you can run the tests like this:\n",
    "\n",
    "```bash\n",
    "pytest 01_intro/first_test.py\n",
    "```\n",
    "\n",
    "And its output should look more or less like this:\n",
    "\n",
    "```bash\n",
    "=============================== test session starts ===============================\n",
    "platform win32 -- Python 3.7.3, pytest-5.1.2, py-1.8.0, pluggy-0.12.0\n",
    "rootdir: C:\\...\\software-engineering-workshop\\testing_exercises\n",
    "plugins: pyfakefs-3.6, cov-2.7.1\n",
    "collected 3 items               \n",
    "\n",
    "01_intro\\first_test.py ...                                                   [100%]\n",
    "\n",
    "================================ 3 passed in 0.03s ================================\n",
    "```\n",
    "\n",
    "You can also pass a full directory with test files, and by default, pytest will look for all available tests inside the current directory (or any of its subdirectories), meaning that if you had run ```pytest 01_intro``` the result would have been the same.\n",
    "\n",
    "With pytest, you can write your tests inside a class, so that they are logically grouped together, or you can just have them inside the same file. Each has its pros and cons. For the class, it gives room for reusing other libraries such as the unittest, which provides a TestCase class where you can easily have your setUp and tearDown methods, but on the other hand, you might have issues when using things like parametrized tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parametrized tests\n",
    "\n",
    "Consider this class:\n",
    "\n",
    "```python\n",
    "from typing import Union\n",
    "\n",
    "Number = Union[int, float]\n",
    "class Calculator:\n",
    "    def sum(self, x: Number, y: Number) -> Number:\n",
    "        return x + y\n",
    "```\n",
    "\n",
    "And a few tests we have created for it:\n",
    "\n",
    "```python\n",
    "def test_sum_1_and_2():\n",
    "    x = 1\n",
    "    y = 2\n",
    "    expected_result = 3\n",
    "    \n",
    "    result = Calculator().sum(x, y)\n",
    "\n",
    "    assert result == expected_result, \\\n",
    "        f\"{x} + {y} is {expected_result}, but the actual result was {result}\"\n",
    "\n",
    "def test_sum_3_and_0():\n",
    "    x = 3\n",
    "    y = 0\n",
    "    expected_result = 3\n",
    "    \n",
    "    result = Calculator().sum(x, y)\n",
    "\n",
    "    assert result == expected_result, \\\n",
    "        f\"{x} + {y} is {expected_result}, but the actual result was {result}\"\n",
    "\n",
    "def test_sum_minus_5_and_2():\n",
    "    # ...\n",
    "\n",
    "def test_sum_minus_5_and_minus_2():\n",
    "    # ...\n",
    "```\n",
    "\n",
    "If you think of it, the only difference between every test is just the values of ```x```, ```y``` and the ```expected_result```, so it would be handy to have a function to which we can pass these parameters and that would run the test. A first implementation of that could look like this:\n",
    "\n",
    "```python\n",
    "def test_sums():\n",
    "    sum_data = [\n",
    "        #x, y, expected_result\n",
    "        (1, 2, 3),\n",
    "        (3, 0, 3),\n",
    "        (-5, 2, -3),\n",
    "        (-5, -2, -7),\n",
    "    ]\n",
    "    \n",
    "    for x, y, expected_result in sum_data:\n",
    "        result = Calculator().sum(x, y)\n",
    "\n",
    "        assert result == expected_result, \\\n",
    "            f\"{x} + {y} is {expected_result}, but the actual result was {result}\"\n",
    "```\n",
    "\n",
    "You can run this example with:\n",
    "\n",
    "```bash\n",
    "pytest 02_parametrized_test/single_test.py\n",
    "```\n",
    "\n",
    "This approach would work, but has some issues:\n",
    "\n",
    "- Everything runs as a single test (i.e. you don't have one test for each of the input rows).\n",
    "- As a consequence, if one of the assertion fails (or one of the calls throws an exception, etc), we might not be testing everything.\n",
    "\n",
    "That's where parametrized tests come in. It will look similar to our ```test_sums``` function, but we factor the for loop out of it. Pytest provides this feature, so our previous test can be rewritten like this:\n",
    "\n",
    "```python\n",
    "import pytest\n",
    "\n",
    "SUM_FIXTURES = [\n",
    "    (1, 2, 3),\n",
    "    (1, 5, 6),\n",
    "    (-5, 4, -1),\n",
    "]\n",
    "\n",
    "@pytest.mark.parametrize(\"x,y,expected_result\", SUM_FIXTURES)\n",
    "def test_sum(x, y, expected_result):\n",
    "    result = Calculator().sum(x, y)\n",
    "\n",
    "    assert result == expected_result, \\\n",
    "        f\"{x} + {y} is {expected_result}, but the actual result was {result}\"\n",
    "```\n",
    "\n",
    "Again, you can run this example with:\n",
    "\n",
    "```bash\n",
    "pytest 02_parametrized_test/parametrized_test.py\n",
    "```\n",
    "\n",
    "The main benefit of this is that in our output, we get one test for each element inside ```SUM_FIXTURES```, so if one of the cases there fails, the rest are of cases are still test and marked as succeed / fail accordingly. Let's say we add ```(1, 4, 6)``` to our ```SUM_FIXTURES```. The test for that case will fail miserably, but the rest will still succeed:\n",
    "\n",
    "```bash\n",
    "=============================== test session starts ===============================\n",
    "platform win32 -- Python 3.7.3, pytest-5.1.2, py-1.8.0, pluggy-0.12.0\n",
    "rootdir: C:\\...\\software-engineering-workshop\\testing_exercises\n",
    "plugins: pyfakefs-3.6, cov-2.7.1\n",
    "collected 4 items\n",
    "\n",
    "02_parametrized_test\\parametrized_test.py ..F.                                 [100%]\n",
    "\n",
    "==================================== FAILURES =====================================\n",
    "_________________________________ test_sum[1-4-6] _________________________________\n",
    "\n",
    "x = 1, y = 4, expected_result = 6\n",
    "\n",
    "    @pytest.mark.parametrize(\"x,y,expected_result\", SUM_FIXTURES)\n",
    "    def test_sum(x, y, expected_result):\n",
    "        result = Calculator().sum(x, y)\n",
    "    \n",
    ">       assert result == expected_result, \\\n",
    "            f\"{x} + {y} is {expected_result}, but the actual result was {result}\"\n",
    "E       AssertionError: 1 + 4 is 6, but the actual result was 5\n",
    "E       assert 5 == 6\n",
    "\n",
    "02_parametrized_test\\parametrized_test.py:21: AssertionError\n",
    "=========================== 1 failed, 3 passed in 0.14s ===========================\n",
    "```\n",
    "\n",
    "If we had done it in just one test method instead, we would get 1 failure and 0 passed, and we wouldn't be sure of how many of the test cases are failing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixtures (and mocks)\n",
    "\n",
    "In most real applications, your classes and functions will have dependencies. One of the key parts to keep your tests maintainable is isolation, meaning that if you have class B, that depends on class A, your tests for B should not depend on the real behavior of A. To do that, we will be introducing Mock objects, as well as another important feature of pytest: test fixtures.\n",
    "\n",
    "### Scenario\n",
    "\n",
    "To set the context, consider the two classes below:\n",
    "\n",
    "```python\n",
    "class TaxCalculator:\n",
    "    def __init__(self, tva: float = .21):\n",
    "        self.tva = tva\n",
    "\n",
    "    def add_taxes(self, amount: float) -> float:\n",
    "        return amount * (1 + self.tva)\n",
    "\n",
    "class Bill:\n",
    "    def __init__(self, tax_calculator: \"TaxCalculator\"):\n",
    "        self.tax_calculator = tax_calculator\n",
    "        self.amount: float = 0.\n",
    "\n",
    "    def add(self, amount: float):\n",
    "        self.amount += amount\n",
    "    \n",
    "    @property\n",
    "    def total(self) -> float:\n",
    "        \"\"\"Calculates the total amount, including taxes\"\"\"\n",
    "        return self.tax_calculator.add_taxes(self.amount)\n",
    "```\n",
    "\n",
    "When testing it, you might think of doing something like this:\n",
    "\n",
    "```python\n",
    "def test_total():\n",
    "    tax_calculator = TaxCalculator(tva=.1)\n",
    "    bill = Bill(tax_calculator=tax_calculator)\n",
    "    bill.add(100.0)\n",
    "    expected_total = 110.0\n",
    "    \n",
    "    total = bill.total()\n",
    "    \n",
    "    assert total == expected_total, f\"Total was {total}, but it should be {expected_total}\"\n",
    "```\n",
    "\n",
    "While this might work, this approach introduces a few problems:\n",
    "- Our tests are not isolated anymore: if the TaxCalculator implementation changes, our tests might start breaking. When testing the Bill class, we don't want to depend on the logic implemented inside the TaxCalculator class.\n",
    "- We will have to create the ```TaxCalculator``` for every test where we want to use it. This means that if the way we create a TaxCalculator object changes, we'll have to change all our tests.\n",
    "\n",
    "To solve the first problem, we'll be using Mock objects: objects that mimic the behaviour of other objects, but that have no actual logic in them.\n",
    "\n",
    "To solve the second problem, we'll use fixtures.\n",
    "\n",
    "Again, you can run this example yourself. If you're inside the ```testing_exercises``` folder, just run:\n",
    "\n",
    "```bash\n",
    "pytest 03_fixtures/01_bill_calculator.py\n",
    "\n",
    "=========================== test session starts ===========================\n",
    "platform win32 -- Python 3.7.3, pytest-5.1.2, py-1.8.0, pluggy-0.12.0\n",
    "rootdir: C:\\...\\software-engineering-workshop\\testing_exercises\n",
    "plugins: pyfakefs-3.6, cov-2.7.1\n",
    "collected 1 item                                                           \n",
    "\n",
    "03_fixtures\\01_bill_calculator.py .                                  [100%]\n",
    "\n",
    "============================ 1 passed in 0.05s ============================\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mocks\n",
    "\n",
    "As we just explained, in order to keep our tests for the ```Bill``` class isolated, we need a way to not depend on the ```TaxCalculator``` class. We can do that by using the [```unittest.mock```](https://docs.python.org/3/library/unittest.mock.html) library. With it, you can create objects that mimic other objects and/or functions.\n",
    "\n",
    "To create a mock, there are two main classes: ```Mock``` and ```MagicMock```. The difference between the two is that ```MagicMock``` implements Python's magic methods, such as ```__str__```.\n",
    "\n",
    "#### Creating a mock\n",
    "\n",
    "To create a mock, we just need to instantiate the MagicMock class:\n",
    "\n",
    "```python\n",
    "from unittest import mock\n",
    "\n",
    "tax_calculator = mock.MagicMock()\n",
    "```\n",
    "\n",
    "Now that we have a mock, we can arbitrarily decide what calling a function will return. We'll see different ways of defining this return values.\n",
    "\n",
    "#### Return a constant value\n",
    "\n",
    "The simplest way is to have a method always return the same value:\n",
    "\n",
    "```python\n",
    "tax_calculator.add_taxes.return_value = 110.0\n",
    "```\n",
    "\n",
    "With this, anytime the ```add_taxes``` method of our mock is called, ```110.0``` will be returned, no matter the arguments passed to the function.\n",
    "\n",
    "#### Return different values in successive calls\n",
    "\n",
    "Sometimes, you want the method to return value \"x\" on the first call, \"y\" on the second call, etc. For this, we can use the ```side_effect``` property, passing a list of values to it: \n",
    "\n",
    "```python\n",
    "tax_calculator.add_taxes.side_effect = [100.0, 200.0, 50.0]\n",
    "```\n",
    "\n",
    "This will return 100.0 on the first call to ```tax_calculator.add_taxes```, 200.0 on the second one and 50.0 on the third one. **IMPORTANT**: If the method is called a fourth time, an exception will be raised!\n",
    "\n",
    "#### Calculating the return value dynamically\n",
    "\n",
    "Finally, you can also decide the return value using your own function, and passing it to the side_effect:\n",
    "\n",
    "```python\n",
    "def add_taxes_side_effect(amount: float):\n",
    "    return amount + 10\n",
    "\n",
    "tax_calculator.add_taxes.side_effect = add_taxes_side_effect\n",
    "```\n",
    "\n",
    "#### Asserting calls to a mock method\n",
    "\n",
    "Finally, it is a common practice in unit tests to assert that the mock methods you expect to be called have actually been called, and that they have been called with the right values. There is a whole range of methods to make different assertions, for instance: ```assert_called``` to just check that it has been called, no matter with what parameters, ```assert_called_once``` to assert that it was called once and only once, ```assert_any_call``` to assert that at least one of the calls to the mock has been made with the arguments we pass, etc. You can find the documentation [here](https://docs.python.org/3/library/unittest.mock.html#unittest.mock.Mock.assert_called)\n",
    "\n",
    "#### Rewriting our test with mocks\n",
    "\n",
    "Now that we have see how to create mocks, we can rewrite our test like this:\n",
    "\n",
    "```python\n",
    "import pytest\n",
    "from unittest import mock\n",
    "\n",
    "def test_total():\n",
    "    tax_calculator = mock.MagicMock()\n",
    "    bill = Bill(tax_calculator=tax_calculator)\n",
    "    amount_to_add = 100.0\n",
    "    amount_after_taxes = 110.0\n",
    "    bill.add(amount_to_add)\n",
    "    tax_calculator.add_taxes.return_value = amount_after_taxes\n",
    "    \n",
    "    total = bill.total\n",
    "\n",
    "    assert total == amount_after_taxes, f\"Total was {total}, but it should be {expected_total}\"\n",
    "    tax_calculator.add_taxes.assert_called_once_with(amount_to_add)\n",
    "```\n",
    "\n",
    "You can run the test with:\n",
    "\n",
    "```bash\n",
    "pytest 03_fixtures/02_bill_calculator_magic_mock.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixtures\n",
    "\n",
    "The last piece we'll introduce now are fixtures. Fixtures allow you to inject dependencies so that you can have full control over them. They include not only mocks but any other thing you might want to reuse across tests. You can find the full documentation for fixtures in the [pytest docs](https://docs.pytest.org/en/latest/fixture.html)\n",
    "\n",
    "#### Defining fixtures\n",
    "\n",
    "You can create a fixture with the ```pytest.fixture``` decorator:\n",
    "\n",
    "```python\n",
    "import pytest\n",
    "\n",
    "@pytest.fixture\n",
    "def tax_calculator():\n",
    "    tax_calculator = mock.MagicMock()\n",
    "    \n",
    "    return tax_calculator\n",
    "```\n",
    "\n",
    "#### Using fixtures\n",
    "\n",
    "To use fixtures in a test, you simply need to add an argument to your test with the fixture name:\n",
    "\n",
    "```python\n",
    "def my_test(tax_calculator):\n",
    "    # do something\n",
    "```\n",
    "\n",
    "#### Available fixtures\n",
    "\n",
    "Pytest already has some [built-in fixtures](https://pytest.readthedocs.io/en/latest/builtin.html#builtin-fixtures-function-arguments) (e.g. to capture logs, the standard output..), and the pyfakefs library we have also adds a ```fs``` fixture that you can use in your tests.\n",
    "\n",
    "If you want to check what fixtures are available for all tests, you can run this:\n",
    "\n",
    "```python\n",
    "pytest --fixtures\n",
    "```\n",
    "\n",
    "Now, some files might define their own fixtures. You can check the fixtures available for a specific file by also providing the name of the file:\n",
    "\n",
    "```python\n",
    "pytest --fixtures my_file.py\n",
    "```\n",
    "\n",
    "#### Sharing fixtures across multiple tests\n",
    "\n",
    "Sometimes, you don't want to define a fixture only for one test file, but you want one that you can reuse through any test. To do that, you can create a ```conftest.py``` file with the fixture, and any test that's on the same directory (or in a subdirectory) will be able to use the fixture.\n",
    "\n",
    "#### Fixture scope\n",
    "\n",
    "A detail that can be important when writing tests is the scope of fixtures. By default, every test case \n",
    "using the fixture will instantiate it again (i.e. the fixture function will be called again). This is ok in many cases, but it's probably not what you want if your fixtures take a long time to be created (e.g. if you want to create a spark session); this is where the scope of fixtures comes into play. You can define the scope of your fixture by passing it to the decorator:\n",
    "\n",
    "```python\n",
    "import pytest\n",
    "from unittest import mock\n",
    "\n",
    "@pytest.fixture(scope=\"session\")\n",
    "def my_session_fixture():\n",
    "    return mock.MagicMock()\n",
    "\n",
    "@pytest.fixture(scope=\"class\")\n",
    "def my_class_fixture():\n",
    "    return mock.MagicMock()\n",
    "```\n",
    "\n",
    "The possible values for ```scope``` are:\n",
    "\n",
    "- **function** (default value): You fixture will be instantiated everytime it's used.\n",
    "- **class**: The fixture is only instantiated once per class. So, if you have classes \"ATest\" and \"BTest\", your fixture will be instantiated twice.\n",
    "- **module**: The fixture is instantiated once per module (i.e. per subdirectory)\n",
    "- **package**: In this case, the fixture is instantiated once per package. Note that a package can have multiple modules.\n",
    "- **session**: This is the \"broadest\" level. The fixture will just be instantiated once.\n",
    "\n",
    "More information:\n",
    "https://docs.pytest.org/en/latest/fixture.html#scope-sharing-a-fixture-instance-across-tests-in-a-class-module-or-session\n",
    "\n",
    "#### Adding a tax_calculator fixture\n",
    "\n",
    "Finally, let's add a tax_calculator fixture to our test.\n",
    "\n",
    "You can run the example with:\n",
    "\n",
    "```bash\n",
    "pytest 03_fixtures/03_bill_calculator_fixtures.py --capture=no\n",
    "```\n",
    "\n",
    "There are two tests using the fixture, so you should see twice the line \"Instantiating tax_calculator fixture\". There is another example that uses the ```module``` scope, that you can also run:\n",
    "\n",
    "```bash\n",
    "pytest 03_fixtures/04_scoped_fixtures.py --capture=no\n",
    "```\n",
    "\n",
    "**IMPORTANT**: Be careful with the scope of the fixtures and the assertions. For instance, if you have a fixture with scope \"module\" and an assertion of the type \"my_fixture.some_method.assert_called_once()\" in more that one test, your assertions might fail, since the call counts for the mock will not be reset between tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
